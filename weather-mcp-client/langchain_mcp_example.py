#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LangChain MCPé€‚é…å™¨ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨langchain-mcp-adaptersåº“è¿æ¥MCPæœåŠ¡å™¨ï¼Œ
åŠ è½½å·¥å…·ï¼Œå¹¶ä½¿ç”¨LangChain Agentå¤„ç†ç”¨æˆ·æŸ¥è¯¢ã€‚
"""

import os
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import BaseTool
from langchain_core.runnables import RunnablePassthrough

# LangChainç»„ä»¶
from langchain.agents import AgentExecutor
from langchain.agents.format_scratchpad import format_to_openai_function_messages
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser

# OpenAIé›†æˆ
from langchain_openai import ChatOpenAI

# MCPé€‚é…å™¨
from langchain_mcp_adapters import connect_to_mcp_server, load_mcp_tools
from langchain_mcp_adapters.tools import MCPToolSpec, convert_mcp_tool_to_langchain

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LangChainMCPExample:
    """
    ä½¿ç”¨LangChainé€‚é…å™¨å¤„ç†MCPå·¥å…·çš„ç¤ºä¾‹ç±»
    
    è¿™ä¸ªç±»å±•ç¤ºäº†å¦‚ä½•:
    1. è¿æ¥åˆ°MCPæœåŠ¡å™¨
    2. åŠ è½½MCPå·¥å…·
    3. å°†MCPå·¥å…·è½¬æ¢ä¸ºLangChainå·¥å…·
    4. åˆ›å»ºLangChain Agent
    5. å¤„ç†ç”¨æˆ·æŸ¥è¯¢
    """
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–LangChain MCPç¤ºä¾‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.server_configs = {}
        self.mcp_client = None
        self.tools = []
        self.agent_executor = None
        self.memory = []  # ä¿å­˜å¯¹è¯å†å²
        
    async def load_server_configs(self) -> Dict:
        """
        åŠ è½½æœåŠ¡å™¨é…ç½®
        
        Returns:
            åŒ…å«æœåŠ¡å™¨é…ç½®çš„å­—å…¸
        """
        try:
            config_path = Path(self.config_path)
            if not config_path.exists():
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
                
            with open(config_path, 'r', encoding='utf-8') as f:
                self.server_configs = json.load(f)
                logger.info(f"å·²åŠ è½½MCPæœåŠ¡å™¨é…ç½®: {self.config_path}")
                return self.server_configs
        except Exception as e:
            logger.error(f"åŠ è½½æœåŠ¡å™¨é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    async def connect_and_load_tools(self, server_name: str) -> List[BaseTool]:
        """
        è¿æ¥åˆ°MCPæœåŠ¡å™¨å¹¶åŠ è½½å·¥å…·
        
        Args:
            server_name: è¦è¿æ¥çš„æœåŠ¡å™¨åç§°
            
        Returns:
            è½¬æ¢åçš„LangChainå·¥å…·åˆ—è¡¨
        """
        if not self.server_configs:
            await self.load_server_configs()
            
        if server_name not in self.server_configs:
            raise ValueError(f"æ‰¾ä¸åˆ°æœåŠ¡å™¨é…ç½®: {server_name}")
            
        server_config = self.server_configs[server_name]
        logger.info(f"æ­£åœ¨è¿æ¥åˆ°MCPæœåŠ¡å™¨: {server_name}")
        
        try:
            # ä½¿ç”¨langchain-mcp-adaptersè¿æ¥åˆ°MCPæœåŠ¡å™¨
            self.mcp_client = await connect_to_mcp_server(server_config)
            logger.info(f"å·²æˆåŠŸè¿æ¥åˆ°MCPæœåŠ¡å™¨: {server_name}")
            
            # åŠ è½½MCPå·¥å…·
            mcp_tools = await load_mcp_tools(self.mcp_client)
            logger.info(f"å·²ä»MCPæœåŠ¡å™¨åŠ è½½å·¥å…·: {len(mcp_tools)}ä¸ªå·¥å…·")
            
            # å°†MCPå·¥å…·è½¬æ¢ä¸ºLangChainå·¥å…·
            self.tools = [convert_mcp_tool_to_langchain(tool) for tool in mcp_tools]
            logger.info(f"å·²è½¬æ¢MCPå·¥å…·ä¸ºLangChainå·¥å…·: {len(self.tools)}ä¸ªå·¥å…·")
            
            return self.tools
        except Exception as e:
            logger.error(f"è¿æ¥MCPæœåŠ¡å™¨æˆ–åŠ è½½å·¥å…·æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def setup_agent(self, model_name: str = "gpt-4o", temperature: float = 0) -> AgentExecutor:
        """
        è®¾ç½®LangChain Agent
        
        Args:
            model_name: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹æ¸©åº¦å‚æ•°
            
        Returns:
            é…ç½®å¥½çš„AgentExecutor
        """
        if not self.tools:
            raise ValueError("è¯·å…ˆåŠ è½½å·¥å…·å†è®¾ç½®Agent")
            
        # åˆ›å»ºLLM
        llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨å„ç§å·¥å…·å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶æ‰§è¡Œã€‚"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # ç»‘å®šLLMå’Œå·¥å…·
        agent = (
            {
                "input": RunnablePassthrough(),
                "chat_history": lambda _: self.memory,
                "agent_scratchpad": lambda x: format_to_openai_function_messages(x["intermediate_steps"]),
            }
            | prompt
            | llm.bind(functions=[tool.metadata for tool in self.tools])
            | OpenAIFunctionsAgentOutputParser()
        )
        
        # åˆ›å»ºAgentæ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=self.tools,
            verbose=True,
        )
        
        logger.info("å·²æˆåŠŸè®¾ç½®Agent")
        return self.agent_executor
    
    async def process_query(self, query: str) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if not self.agent_executor:
            raise ValueError("è¯·å…ˆè®¾ç½®Agentå†å¤„ç†æŸ¥è¯¢")
            
        # è®°å½•ç”¨æˆ·æ¶ˆæ¯
        self.memory.append(HumanMessage(content=query))
        
        # æ‰§è¡ŒæŸ¥è¯¢
        try:
            result = await self.agent_executor.ainvoke({"input": query, "intermediate_steps": []})
            response = result["output"]
            
            # è®°å½•AIå›å¤
            self.memory.append(AIMessage(content=response))
            
            return response
        except Exception as e:
            error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            return error_msg


async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = LangChainMCPExample("config.json")
    
    try:
        # è¿æ¥åˆ°MCPæœåŠ¡å™¨å¹¶åŠ è½½å·¥å…·
        await client.connect_and_load_tools("weather")
        
        # è®¾ç½®Agent
        client.setup_agent()
        
        print("ğŸ“ MCPå·¥å…·å·²åŠ è½½å®Œæˆï¼Œå¯ä»¥å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰")
        
        # å¤„ç†ç”¨æˆ·è¾“å…¥
        while True:
            user_input = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
            if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            response = await client.process_query(user_input)
            print(f"ğŸ¤– {response}")
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
    finally:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†ä»£ç 
        if client.mcp_client:
            await client.mcp_client.close()


if __name__ == "__main__":
    asyncio.run(main()) 